% ========================== INTRODUCTION ==========================================
\section{Introduction}
\label{sec:introduction}
The product data cycle of a typical consumer web company consists of a continuous cycle of three phases - data collection, processing and finally serving. The data collection phase deals with gathering raw events like user activity, shares, etc. from various storage solutions as well as logging systems. The ability to horizontally scale this layer with the increase in number of data sources is a tough problem and has been solved in the form of various distributed aggregation systems. Some examples of these include LinkedIn's Kafka\cite{kafka}, Facebook's Scribe\cite{scribe} and Cloudera's Flume\cite{flume}. Besides providing access to numerous data-sources these systems also provide pluggable sink interfaces, in particular the ability to push the data into batch processing systems like Hadoop (in particular HDFS)\cite{hadoop}. The Hadoop ecosystem then in-turn provides a diverse offering of query languages thereby providing the ability to run complex product driven algorithms as batch jobs. The size of the output generated by these jobs can end up being really massive. For example in the context of social networks, most algorithms tend to derive relationships between items (where items could be users or any other important facet). This sparse relationship data can get really huge when dealing with millions of items. The end goal of most of this processing is to surface insights which can then be provided back to the user. For the same we need a system which is capable of consuming this batched data while still serving the existing in-coming requests with minimum performance hit. 

In this paper we talk about our solution for the last phase of this cycle, called Project \projectname{}, and how it fits into our product ecosystem. The tricky part of this system is the efficient movement of large amounts of the data, especially since we have constraints to refresh these `insights' on a near daily basis. At LinkedIn our largest cluster serves multiple TBs of data, while loading around 3 TBs of new data to the site every day. Another important feature that we wanted our system to support was the ability to deliver results under failure scenarios. We also built the system with horizontally scalability in mind so as to support addition of new products immediately. \projectname{} has been running in production at LinkedIn for the past 2 years and has successfully been serving various user-facing features like `People you may know', `LinkedIn Skills' and `Who viewed my profile'. One of the reasons behind its success and quick adoption within the company has been its pluggable architecture which makes it easy to test. Also having a simple key-value based API has allowed our engineers to quickly iterate thereby now resulting in over 100 features (stores) on our largest cluster.

The rest of the paper has been structured into 5 sections. In Section \ref{sec:related_work} we talk about some of the existing solutions and how it compares with \projectname{}'s design. The next section, Section \ref{sec:system_architecture},  talks about our system architecture and its evolution. \projectname{} is heavily inspired from Amazon's Dynamo\cite{dynamo} and was hence initially designed to only support the fast online read/write load. But its extensible storage layer allowed us to quickly build our own custom read-only storage engine and integrate with the offline data cycle. Section \ref{sec:read_only} goes into details about the storage engine specifics and the various design decisions we made while building it. In Section \ref{sec:benchmark} we give details about our experiences while using \projectname{} at LinkedIn, along with some performance results. We finally discuss some of our future work and conclude in Section \ref{sec:conclusion}. 


