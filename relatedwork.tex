\section{Related Work}
\label{sec:related_work}

A very common serving system used in various companies is
MySQL~\cite{mysql}. The two most commonly used storage engines of
MySQL, MyISAM and InnoDB, provide bulk loading capabilities into a
live system with the \sql{LOAD DATA INFILE} statement. MyISAM provides
a compact on-disk structure and the ability to delay re-creation of
the index after the load, which unfortunately requires considerable
memory to maintain a special tree-like cache during bulk loading.
Further, the MyISAM storage engine locks the complete table for the
duration of the load thereby resulting in queued requests. On the
other hand, InnoDB supports row-level locking, but its on-disk
structure requires considerable disk space and its bulk loading is
order of magnitudes slower than MyISAM. 

There is considerable work done to add bulk loading ability to new
shared-nothing cluster~\cite{sharednothing} databases similar to
\projectname{}. \citet{silberstein} introduces the problem of bulk
inserting into range partitioned tables in PNUTS~\cite{pnuts}, which
tries to optimize data movement between machines and total transfer
time by adding an extra planning phase to gather statistics and
prepare the system for incoming workload. An extension of that
work~\citet{pnutsbatch} uses Hadoop to batch insert data into PNUTS in
the reduce phase. Both of these approaches optimize the time for data
loading into the live system, but incur latency degradation on live
serving due to multitenant issues with sharing CPU and memory during
the full loading process. This is a signifcant problem with very large
data-sets, which even after optimizations, may take hours to load.  

\projectname{} can alleviate this problem by offloading the
construction of the indexes to an offline system. Usage of MapReduce
for this offline construction has implementations in systems such as
various search systems, like Katta~\cite{katta} and \citet{mika}.
These search layers trigger builds on Hadoop to generate indexes, and
on completion pull the indices to serve search requests. 

This approach has also been extended to various databases.
\citet{konstantinou} and \citet{barbuzzi} suggest building HFiles
offline in Hadoop, then shipping them to HBase~\cite{hbase}, an
open-source database modelled after BigTable~\cite{bigtable}. These
works do not explore the data pipeline, particularly data refreshes
and rollback. More importantly, since the data itself is immutable,
\projectname{} makes optimizations at the storage layer level.

From an architecture perspective, \projectname{} has been inspired
from various previous DHT storage systems. Unlike the previous DHT
systems, such as Chord~\cite{chord}, which provide $O(log~N)$ lookup,
\projectname{}'s lookups are~$O(1)$ since the complete cluster
topology is stored on every node. This allows clients to bootstrap
from a random node and then direct requests to exact destination
nodes. Similar to Dynamo~\cite{dynamo}, \projectname{} also supports
per-tuple based replication for availability purposes. Updating all of
these replicas is effortless in the batch scenario since they are
pre-computed and then loaded into a \projectname{} cluster at once. 
