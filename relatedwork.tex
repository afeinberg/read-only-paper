\section{Related Work}
\label{sec:related_work}
A very common serving system used in various companies is MySQL~\cite{mysql}. The two most used storage engines of MySQL - MyISAM~\cite{myisam} and InnoDB~\cite{innodb} - provide bulk loading capabilities into a live system with the \sql{LOAD DATA INFILE} statement. MyISAM provides a compact on-disk structure and the ability to delay re-creation of the index after the load~\cite{bulk}.  Unfortunately it requires considerable amount of memory to maintain a special tree-like cache during bulk loading. 
%*\cite{http://dev.mysql.com/doc/refman/5.1/en/server-system-variables.html-sysvar_bulk_insert_buffer_size}
Further MyISAM storage engine locks the complete table for the duration of the load thereby resulting in queued requests. On the other hand, InnoDB's on-disk structure has a massive disk utilization. Also bulk loading into InnoDB is order of magnitudes slower than MyISAM. 
%*\cite{http://blogs.oracle.com/carriergrademysql/entry/tips_for_bulk_loading}

There is considerable work done to add bulk loading ability to new shared-nothing cluster\cite{sharednothing} databases similar to \projectname{}. \cite{silberstein} tackles the problem of bulk insertion into range partitioned tables in PNUTS~\cite{pnuts} by adding another \emph {planning phase} to gather more statistics for the movement. Another paper from the same team \cite{pnutsbatch} shows how they used Hadoop to batch insert data faster into PNUTS. These approaches try to optimize the bulk updates on the indexes serving live traffic. 

In \projectname{} we intend to cater to only static data-sets that require a complete change of the full data-set. Hence we can construct the full index offline alleviating the problem of sharing resources (like CPU and memory) while serving the live traffic. Usage of MapReduce for this offline construction has implementations in production systems such as various search systems, like Katta~\cite{katta} and \cite{mika}. These search layers trigger builds on Hadoop to generate indexes, and on completion pull the indices to serve search requests. 

This approach has also been extended to various databases. \cite{konstantinou} and \cite{barbuzzi} suggested building HFiles (equivalent to SSTable files, a persistent immutable map data structure, from BigTable~\cite{bigtable}) offline in Hadoop then shipping them to be consumed immediately by HBase~\cite{hbase}. This unconventional method bypasses the client API alleviating the problems of buffering that affects the write path. The same approach has been adopted by Cassandra~\cite{cassandra} in its new \emph{sstableloader}~\cite{cassandra_bulk} functionality. In general, these approaches alleviate the cluster from sudden memory usage pattern changes. These solutions don't solve one very important use case of ours. In the scenario of a bad data push having the ability to instantaneously rollback to a previous good state is important. The only way to rollback the data for the above approaches would be to re-run the time consuming batch Hadoop job.

From an architecture perspective, \projectname{} has been inspired from various previous DHT storage systems. Unlike the previous DHT systems, like Chord~\cite{chord}, which provide O(log N) lookup, our lookups are O(1) since we store the complete cluster topology on every node. This allows our clients to bootstrap from a random node and then direct requests to exact destination nodes. Similar to Dynamo~\cite{dynamo}, \projectname{} also supports per tuple based replication for availability purposes. Updating all of these replicas is effortless in the batch scenario since they are pre-computed and then loaded into a \projectname{} cluster at once. 

