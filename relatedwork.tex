\section{Related Work}
\label{sec:related_work}
A very common serving system used in various companies is MySQL~\cite{mysql}. The two most used storage engines of MySQL - MyISAM and InnoDB - provide bulk loading capabilities into a live system with the \sql{LOAD DATA INFILE} statement. MyISAM provides a compact on-disk structure and the ability to delay re-creation of the index after the load~\cite{bulk}.  Unfortunately it requires considerable amount of memory to maintain a special tree-like cache during bulk loading. Further MyISAM storage engine locks the complete table for the duration of the load thereby resulting in queued requests. On the other hand, InnoDB's on-disk structure has a massive disk utilization. Also bulk loading into InnoDB is order of magnitudes slower than MyISAM. 

There is considerable work done to add bulk loading ability to new shared-nothing cluster\cite{sharednothing} databases similar to \projectname{}. \cite{silberstein} talks about the problem of bulk inserting into range partitioned tables in PNUTS~\cite{pnuts}. The paper tries to optimize the data being moved between the machines and the total transfer time by adding an extra \emph {planning phase} to gather statistics and prepare the system for incoming workload. Another paper from the same team \cite{pnutsbatch} shows how they used Hadoop to batch insert data faster into PNUTS. Both of these approaches optimize the time for data loading into the live system, but incur latency degradation during the full loading process. This is a problem since \projectname{} caters to very large data-sets, which even after optimizations, may take hours to load.  

The latency degradation in the above solutions is due to the sharing of resources (like CPU and memory) with the live serving traffic. We can alleviate this problem by offloading the construction of the indexes to an offline system. Usage of MapReduce for this offline construction has implementations in systems such as various search systems, like Katta~\cite{katta} and \cite{mika}. These search layers trigger builds on Hadoop to generate indexes, and on completion pull the indices to serve search requests. 

This approach has also been extended to various databases. \cite{konstantinou} and \cite{barbuzzi} suggested building HFiles (equivalent to SSTable files, a persistent immutable map data structure, from BigTable~\cite{bigtable}) offline in Hadoop then shipping them to be consumed immediately by HBase~\cite{hbase}. The same approach has been adopted by Cassandra~\cite{cassandra} in its new \emph{sstableloader}~\cite{cassandra_bulk} functionality. This unconventional method bypasses the client API solving the problems of sudden memory usage pattern changes due to buffering that would have affected the write path. These solutions don't solve one very important use case of ours. In the scenario of a bad data push having the ability to instantaneously rollback to a previous good state is important. The only way to rollback the data for the above approaches would be to re-run the time consuming batch Hadoop job.

From an architecture perspective, \projectname{} has been inspired from various previous DHT storage systems. Unlike the previous DHT systems, like Chord~\cite{chord}, which provide $O(log~N)$ lookup, our lookups are $O(1)$ since we store the complete cluster topology on every node. This allows our clients to bootstrap from a random node and then direct requests to exact destination nodes. Similar to Dynamo~\cite{dynamo}, \projectname{} also supports per tuple based replication for availability purposes. Updating all of these replicas is effortless in the batch scenario since they are pre-computed and then loaded into a \projectname{} cluster at once. 

