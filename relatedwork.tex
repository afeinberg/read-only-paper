\section{Related Work}
\label{sec:related_work}
A very common serving system used in various companies is MySQL. The two most used storage engines of MySQL - MyISAM and InnoDB - provide bulk loading capabilities into live system with `LOAD DATA INFILE' statement. MyISAM is the better amongst the two because of its compact on-disk space usage and its ability to delay the re-creation of the index to a time after the load\cite{bulk}.  Unfortunately we still have to deal with the problem of needing lots of memory, which is used for maintaining the special tree-like cache during bulk loading. 
%*\cite{http://dev.mysql.com/doc/refman/5.1/en/server-system-variables.html-sysvar_bulk_insert_buffer_size}
The other big problem with MyISAM storage engine is that it locks the complete table for the duration of the load. Hence all other requests coming in during the bulk load process are queued up. This is definitely not acceptable for web applications in general since this is equivalent to having a down-time.  

A lot of work has also been done to add bulk loading ability to some new shared-nothing cluster\cite{sharednothing} databases similar to \projectname{}. In \cite{silberstein} tackles the problem of bulk insertion into range partitioned tables in PNUTS \cite{pnuts} by adding another \emph {planning phase} to gather more statistics for the movement. Another paper from the same team \cite{pnutsbatch} shows how they used Hadoop to batch insert data faster into PNUTS.

Most of the approaches above try to optimize the bulk updates on the same indexes which are serving live traffic as well. Since we cater to static data-sets that require a complete change of the full data-set the approach we adopted was the construction of the full index offline. This alleviates the problem of us sharing any resources (like CPU and memory) with the index which is serving the live traffic. Usage of MapReduce for this offline construction has actually been a well researched idea with implementations in various production systems. The first idea of this offline construction originated in the various search systems, like Katta\cite{katta} and \cite{mika}. These search layers have hooks which trigger builds on Hadoop to generate indexes and on completion pull the indices to serve search requests. This idea has also been extended to various databases. Both \cite{konstantinou} and \cite{barbuzzi} suggested building HFiles (equivalent to SSTable files, a persistent immutable map data structure, from Google's BigTable \cite{bigtable}) offline in Hadoop and then shipping them over to be consumed immediately by HBase\cite{hbase}. This unconventional method hence bypasses the client API there-by getting rid of the problems of buffering, etc. which affects the write path. The same approach has been adopted by Cassandra\cite{cassandra} in its new \emph{sstableloader}\cite{cassandra_bulk} functionality. In general these approaches definitely alleviate the cluster from sudden memory usage pattern changes. 

But all of the above solutions don't solve one very important use case of ours. The general development pattern that we've seen for data products is that engineers come up with new algorithm tweaks and then push out the corresponding data changes in steps, while continuously monitoring their metrics. The problem kicks in when a new data load is resulting in a major drop in metrics. In such a scenario having to run a batch Hadoop job to bulk load new data back in would be very time-consuming. Our storage layout tries to solve this problem by providing some form of rollback mechanism. 
 
From an architecture perspective, \projectname{} falls into the league of many previous P2P storage systems. There have been various structured DHTs, like Chord\cite{chord} and Pastry\cite{pastry}, which provide O(log N) lookup. We are more inspired by Dynamo and Beehive\cite{beehive} which tries to decrease the variance in latency due to multi-hop routing by saving more state and providing lookups in O(1). Similar to Dynamo, \projectname{} also supports per tuple based replication for availability purposes. Updating all replicas is easy in the read-only batch scenario since these are pre-computed and then loaded into \projectname{} at once. To make replica updates fast in read-write scenario we allow our updates to some of the replicas of a key to be asynchronous. Network partitions or server failures during these asynchronous updates can result in inconsistencies between replicas. To solve this problem we version every replica and then delegate any resolution to the application during a $get$. This application level resolution has been inspired by previous work done by Bayou\cite{bayou}. 



