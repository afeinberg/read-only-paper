\section{Conclusions \& Future Work}
\label{sec:conclusion}

In this paper we present a low-latency bulk loading system capable of serving multiple TBs of data. By moving the index construction offline to a batch system like Hadoop, we make our serving layer's performance more reliable. \linkedin{} has successfully been running read-only \projectname{} clusters for the past 2 years. It has become an integral part of the product eco-system with various engineers also using it frequently for quick prototypes of products. 

There are still some interesting functionalities that we would like to add to the read-only storage pipeline. Over time we have found that during fetches we exhaust the full bandwidth between data-centers running Hadoop (in particular HDFS) and Voldemort. We therefore need improvements to the push process to reduce network usage with minimum impact on build time. Firstly, we want to explore incremental loads. This can be done by generating patches for the data files on the Hadoop side (by comparing against the snapshot of the previously loaded data still in HDFS) and then applying these on the \projectname{} side during the fetch. We can send the complete index files over since (a) they are relatively small files (b) we want to exploit the OS caching of these files during the fetch. This has not seen many use-cases till now since most of our current stores are recommendation products where the values, represented by floats for probabilities, generally change between iterations for most users. Another important feature that would save bandwidth is the ability to only fetch one replica of the data from HDFS and then propagate it further on the \projectname{} node. 

Finally we would like to explore some more index structures which would make lookups faster and can be built in Hadoop easily. In particular a lot of work has been done in the area of cache oblivious trees, like van Emde Boas trees, which requires no knowledge of page size to get optimal cache performance. 
